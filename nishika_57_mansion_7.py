# -*- coding: utf-8 -*-
"""nishika_57_mansion_7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1V3t3pGlL-onP4crCNchNUBxMtAip9hXd
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install optuna japanize-matplotlib > /dev/null

!pip install catboost

# ご自身のパスを設定
YOUR_PATH = "/content/drive/MyDrive/cpt-house-2024sp"

cd {YOUR_PATH}

# Import necessary libraries
import re
from glob import glob
from functools import partial
from itertools import product
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import lightgbm as lgb
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import KFold
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import FunctionTransformer, LabelEncoder, OrdinalEncoder, StandardScaler
from catboost import CatBoostRegressor
import japanize_matplotlib

sns.set(font="IPAexGothic")

!cat /proc/meminfo | awk '($1=="MemTotal:") {print $2/1048576}'

!python -V

!pip freeze

# Utility functions
def normalize_moyori(moyori):
    if moyori == moyori:  # Check for NaN
        moyori_dict = {'30分?60分': 45, '1H?1H30': 75, '1H30?2H': 105, '2H?': 120}
        moyori = moyori_dict.get(moyori, moyori)
        moyori = int(float(moyori))
    return moyori

def normalize_area(area):
    if area == area:  # Check for NaN
        area = int(re.sub('m\^2未満|㎡以上', '', str(area)))
    return area

def convert_wareki_to_seireki(wareki):
    if wareki == wareki:  # Check for NaN
        if wareki == '戦前':
            wareki = '昭和20年'
        value = wareki[2:-1]
        value = 1 if value == '元' else int(value)
        era_dict = {'昭和': 1925, '平成': 1988, '令和': 2018}
        for era, base_year in era_dict.items():
            if era in wareki:
                seireki = base_year + value
                break
    else:
        seireki = wareki
    return seireki

# Load data
BASE_PATH = './data/'
train_paths = glob(BASE_PATH + 'input/train/*')
train_dfs = [pd.read_csv(path) for path in train_paths]
train_df = pd.concat(train_dfs).reset_index(drop=True)
test_df = pd.read_csv(BASE_PATH + 'input/test.csv')
sub_df = pd.read_csv(BASE_PATH + 'input/sample_submission.csv')

# Initial data cleaning
ID = 'ID'
TARGET = '取引価格（総額）_log'
rm_cols = ['市区町村コード']

df = pd.concat([train_df, test_df])
for col, nunique in df.nunique().items():
    if nunique <= 1:
        rm_cols.append(col)

test_df[TARGET] = np.nan
train_df.drop(rm_cols, axis=1, inplace=True)
test_df.drop(rm_cols, axis=1, inplace=True)

df = pd.concat([train_df, test_df]).sort_values('取引時点').reset_index(drop=True)

# Encode '取引時点'
val_min_idx = min(df[df['取引時点'].str.contains('2022年第4四半期|2023年第1四半期')].index)
test_min_idx = min(df[df['取引時点'].str.contains('2023年第2四半期|2023年第3四半期')].index)
enc_dic = {v: i for i, v in enumerate(sorted(df['取引時点'].unique()))}
df['取引時点_enc'] = df['取引時点'].map(enc_dic)

# Target encoding for '都道府県名'
te_dic = {}
for i in set(df['取引時点_enc'].values):
    tmp_df = df[df['取引時点_enc'] < i]
    te_dic[i] = tmp_df.groupby('都道府県名')[TARGET].mean().to_dict()

df['都道府県名_te'] = df.apply(lambda row: te_dic.get(row['取引時点_enc'], {}).get(row['都道府県名'], 0), axis=1)

# Preprocess other columns
df['取引時点_何年前'] = df['取引時点'].apply(lambda x: 2021 - int(x[:4]))
df['建築年'] = df['建築年'].apply(convert_wareki_to_seireki)
df['面積（㎡）'] = df['面積（㎡）'].apply(normalize_area)
df['最寄駅：距離（分）'] = df['最寄駅：距離（分）'].apply(normalize_moyori)

# 面積と建ぺい率の交互作用特徴量
df['面積×建ぺい率'] = df['面積（㎡）'] * df['建ぺい率（％）']
# 地域の平均価格
df['地域平均価格'] = df.groupby('市区町村名')[TARGET].transform('mean')
# 季節性を捉える特徴量
df['取引月'] = df['取引時点'].apply(lambda x: int(x.split('第')[1][0]))
df['取引季節'] = df['取引月'].apply(lambda x: (x-1)//3 + 1)

# 面積のログ変換
df['面積_log'] = np.log1p(df['面積（㎡）'])

# 特徴量のスケーリング
scaler = StandardScaler()
scaled_features = scaler.fit_transform(df[['面積（㎡）', '建ぺい率（％）', '容積率（％）']])
df[['面積_scaled', '建ぺい率_scaled', '容積率_scaled']] = scaled_features

# Merge latitude and longitude data
lat_lon_df = pd.read_csv(BASE_PATH + 'input/lat_lon.csv')
df['address'] = df['都道府県名'] + " " + df['市区町村名'] + " " + df['地区名']
df = pd.merge(df, lat_lon_df, on='address', how='left').drop(['address', '都道府県名', '地区名'], axis=1)

# Calculate '建築からの年数'
current_year = pd.to_datetime('now').year
df['建築からの年数'] = current_year - df['建築年']
df.drop(['建築年'], axis=1, inplace=True)

# Plot distribution of '建築からの年数'
sns.histplot(df['建築からの年数'])
plt.show()

# Plot correlation matrix
numeric_df = df.select_dtypes(include=['number'])
correlation_matrix = numeric_df.corr()
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix')
plt.show()

# Bin '建築からの年数'
bins = [0, 10, 20, 30, 40, 50, 100]
labels = ['0-10', '11-20', '21-30', '31-40', '41-50', '50+']
df['建築からの年数_カテゴリ'] = pd.cut(df['建築からの年数'], bins=bins, labels=labels, right=False)

# Check for missing values
df.isnull().sum()

# Feature engineering functions
def select_categorical(df):
    return df.select_dtypes(include=['object', 'category'])

def select_numerical(df):
    return df.select_dtypes(include=['int', 'float'])

def encode_labels(df):
    df = df.copy()
    for col in df.columns:
        if df[col].dtype.name == 'category':
            df[col] = df[col].cat.add_categories(['Missing']).fillna('Missing')
        else:
            df[col] = df[col].fillna('Missing')
        df[col] = LabelEncoder().fit_transform(df[col])
    return df

def combinate_yoseki(df):
    df["面積（㎡）容積率（％）_combi"] = df["面積（㎡）"] * df["容積率（％）"]
    return df[["面積（㎡）容積率（％）_combi"]].copy()

# Apply feature engineering
num_df = FunctionTransformer(select_numerical).fit_transform(df)
le_df = make_pipeline(FunctionTransformer(select_categorical, validate=False), FunctionTransformer(encode_labels, validate=False)).fit_transform(df)
num_comb_df = make_pipeline(FunctionTransformer(select_numerical), FunctionTransformer(combinate_yoseki)).fit_transform(df) / 100

# Aggregate features
def get_agg_df(df, group_col):
    agg_cols = ["最寄駅：距離（分）", "面積（㎡）", "建ぺい率（％）", "容積率（％）"]
    cols = [group_col] + agg_cols
    agg_df = df[cols].copy()
    functions = ["count", "mean", "min", "max"]
    for col, function in product(agg_cols, functions):
        agg_df[f"{col}_{function}"] = agg_df.groupby(group_col)[col].transform(function)
    return agg_df.drop(cols, axis=1)

group_col = '市区町村名'
agg_df = get_agg_df(df, group_col)

# Combine all features
feat_df = pd.concat([num_df, le_df, num_comb_df, agg_df], axis=1)

# Split train, validation, and test sets
train_df = feat_df.iloc[:val_min_idx, :]
val_df = feat_df.iloc[val_min_idx:test_min_idx, :]
test_df = feat_df.iloc[test_min_idx:, :]

feat_cols = [col for col in train_df.columns if col not in rm_cols + [ID, TARGET]]
cat_cols = list(le_df.columns) + ['取引時点_enc']

train_x, train_y = train_df[feat_cols], train_df[TARGET]
val_x, val_y = val_df[feat_cols], val_df[TARGET]
test_x, test_y = test_df[feat_cols], test_df[TARGET]

# LightGBM model training
params = {
    'objective': 'regression',
    'metric': 'mae',
    'num_leaves': 42,
    'max_depth': 7,
    "feature_fraction": 0.8,
    'subsample_freq': 1,
    "bagging_fraction": 0.95,
    'min_data_in_leaf': 2,
    'learning_rate': 0.1,
    "boosting": "gbdt",
    "lambda_l1": 0.1,
    "lambda_l2": 10,
    "verbosity": -1,
    "random_state": 43,
    "num_boost_round": 50000,
    "early_stopping_rounds": 100
}

train_data = lgb.Dataset(train_x, label=train_y)
val_data = lgb.Dataset(val_x, label=val_y)

model = lgb.train(
    params,
    train_data,
    categorical_feature=cat_cols,
    valid_names=['train', 'valid'],
    valid_sets=[train_data, val_data],
    callbacks=[lgb.log_evaluation(100)],
)

val_pred_lgb = model.predict(val_x, num_iteration=model.best_iteration)
score = mean_absolute_error(val_y, val_pred_lgb)
print(f'LightGBM score: {score:.4f}')

# CatBoost model training
cb_params = {
    'iterations': 5000,
    'learning_rate': 0.1,
    'depth': 7,
    'loss_function': 'MAE',
    'eval_metric': 'MAE',
    'random_seed': 0,
    'od_type': 'Iter',
    'od_wait': 100,
    'cat_features': cat_cols,
}

cb_model = CatBoostRegressor(**cb_params)
cb_model.fit(train_x, train_y, eval_set=(val_x, val_y), use_best_model=True, verbose=100)

val_pred_cb = cb_model.predict(val_x)
score = mean_absolute_error(val_y, val_pred_cb)
print(f'CatBoost score: {score:.4f}')

# Ensemble model
val_pred_ensemble = (val_pred_cb + val_pred_lgb) / 2
score_ensemble = mean_absolute_error(val_y, val_pred_ensemble)
print(f'Ensemble score: {score:.4f}')

# Feature importance
lgb.plot_importance(model, figsize=(12, 8), max_num_features=50, importance_type='gain')
plt.tight_layout()
plt.savefig(BASE_PATH + 'output/feature_importance.png')
plt.show()

importance_df_cb = pd.DataFrame({'Feature': train_x.columns, 'Importance': cb_model.get_feature_importance()}).sort_values(by='Importance', ascending=False)
plt.figure(figsize=(12, 8))
sns.barplot(x="Importance", y="Feature", data=importance_df_cb.head(50))
plt.title('CatBoost Feature Importance')
plt.tight_layout()
plt.savefig(BASE_PATH + 'output/feature_importance_cb.png')
plt.show()

# Predict test data
test_pred_cb = cb_model.predict(test_x)
test_pred_lgb = model.predict(test_x, num_iteration=model.best_iteration)
test_pred_ensemble = (test_pred_cb + test_pred_lgb) / 2

test_df[TARGET] = test_pred_ensemble
sub_df = pd.merge(sub_df[['ID']], test_df[['ID', TARGET]], on='ID')
sub_df.to_csv(BASE_PATH + 'output/test_submission.csv', index=False)